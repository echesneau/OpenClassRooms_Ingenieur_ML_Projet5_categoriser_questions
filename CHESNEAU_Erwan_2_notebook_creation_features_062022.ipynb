{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH01KhRFh1p-"
   },
   "source": [
    "# Projet 5 : Catégorisez automatiquement des questions\n",
    "## Contexte et objectifs\n",
    "Le site Stackoverflow permet de poser des questions sur le thème de la programmation informatique. Afin de classifier les questions, les utilisateurs doivent renseigner des tags afin de retrouver plus facilement les questions. Afin d'aider les utilisateurs, le but du projet est de proposer des suggestions de tags en fonction du contenu de la question.  \n",
    "Après avoir exploré les données et tester différents modèles pour segmenter les données, un code sera déployer afin de créer une API utilisable par Stackoverflow\n",
    "## Notebook de création des features pour la segmentation\n",
    "Dans ce notebook les différentes features qui seront utilisées pour la ségmentation sont créées et sauvegardées.  \n",
    "La plupart des fonctions utilisées ont été soit directement copiées du notebook présenté dans le projet, soit elles en sont fortement inspirées.  \n",
    "Les fonctions du modèle BERT ont été fortement modifiées pour la plupart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUcqyHs1h3sT"
   },
   "source": [
    "## Modules Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66898,
     "status": "ok",
     "timestamp": 1655202444918,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "6tJQgiwKPEGE",
    "outputId": "c0e8944b-552f-4050-b1f7-c92168c82572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==4.1.2 in c:\\users\\erwan\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\erwan\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\erwan\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\erwan\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (6.0.0)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\erwan\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (0.29.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erwan\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "C:\\Users\\erwan\\anaconda3\\lib\\site-packages\\fsspec\\spec.py:92: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if pa_version and LooseVersion(pa_version) < LooseVersion(\"2.0\"):\n",
      "C:\\Users\\erwan\\anaconda3\\lib\\site-packages\\fsspec\\spec.py:92: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if pa_version and LooseVersion(pa_version) < LooseVersion(\"2.0\"):\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\erwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\erwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\erwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\erwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# module général\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tokenization # Class pour la tokenisation disponible dans Tensorflow\n",
    "\n",
    "#module SKlearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# module pyLDAvis : visualisation LDA\n",
    "try : \n",
    "    import pyLDAvis.sklearn\n",
    "except : \n",
    "    !pip install pyLDAvis\n",
    "    import pyLDAvis.sklearn\n",
    "\n",
    "# module gensim\n",
    "!pip install gensim==4.1.2\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.models\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "try :\n",
    "    import sacremoses\n",
    "except :   \n",
    "    !pip install sacremoses\n",
    "    import sacremoses\n",
    "try :\n",
    "    import transformers\n",
    "except :\n",
    "    !pip install transformers\n",
    "    import transformers\n",
    "from transformers import *\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# téléchargement nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21064,
     "status": "ok",
     "timestamp": 1655202465975,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "hEdz69OJmHjJ",
    "outputId": "d91468ec-d4a2-441c-b3cf-38c97ddeb2b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# pour google colab uniquement\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdAk9rSMu5Hs"
   },
   "source": [
    "## fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRt_gaCymYL6"
   },
   "outputs": [],
   "source": [
    "def tokenize_lemmat(txt) :\n",
    "    tag_map = defaultdict(lambda : nltk.corpus.wordnet.NOUN)\n",
    "    tag_map['J'] = nltk.corpus.wordnet.ADJ\n",
    "    tag_map['V'] = nltk.corpus.wordnet.VERB\n",
    "    tag_map['R'] = nltk.corpus.wordnet.ADV\n",
    "\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tag_tokenizer = nltk.RegexpTokenizer(r'</?(?:b|p)>', gaps=True)\n",
    "    txt_tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    txt = ''.join([i for i in txt if not i.isdigit()])\n",
    "    txt = re.sub(r'_+', ' ', txt)\n",
    "    words = txt_tokenizer.tokenize(' '.join(tag_tokenizer.tokenize(txt.lower())))\n",
    "    out = [lemmatizer.lemmatize(token, tag_map[tag[0]]) for token, tag in nltk.pos_tag(words)]\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4YmcvEahGqb"
   },
   "outputs": [],
   "source": [
    "def tokenize_simple(txt) :\n",
    "    tag_tokenizer = nltk.RegexpTokenizer(r'</?(?:b|p)>', gaps=True)\n",
    "    txt_tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    txt = ''.join([i for i in txt if not i.isdigit()])\n",
    "    txt = re.sub(r'_+', ' ', txt)\n",
    "    \n",
    "    tokens = txt_tokenizer.tokenize(' '.join(tag_tokenizer.tokenize(txt.lower())))\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5BkPttNu7AH"
   },
   "outputs": [],
   "source": [
    "def dummy(doc) :\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iffN1Rdi7kOU"
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vNplZ4_8WGB"
   },
   "outputs": [],
   "source": [
    "def do_lda(docs, max_df=1, min_df=1., max_features=1000, n_topics=5):\n",
    "    def display_topics(model, feature_names, no_top_words):\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            print(\"Topic {}:\".format(topic_idx))\n",
    "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    #tf_vectorizer = CountVectorizer(lowercase=True, tokenizer=tokenize_body, max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english')\n",
    "    tf_vectorizer = CountVectorizer(tokenizer=dummy, preprocessor=dummy, max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(docs)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=42)\n",
    "    lda.fit(tf)\n",
    "    n_top_words = 20\n",
    "    display_topics(lda, tf_vectorizer.get_feature_names_out(), n_top_words)\n",
    "    #coherence_model_lda = CoherenceModel(model=lda, texts=docs, dictionary=id2word, coherence='c_v')\n",
    "    #coherence_lda = coherence_model_lda.get_coherence()\n",
    "    #print('\\nCoherence Score: ', coherence_lda)\n",
    "    return lda, tf, tf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UET71xzMjQRv"
   },
   "source": [
    "## Chargement des données\n",
    "les données nettoyées dans le notebook précédent sont rechargées.  \n",
    "Le Chargement est effectué depuis le fichier pickle pour éviter le traitement nécessaire pour considérer les colonnes de type list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyCV5cSZmOM0"
   },
   "outputs": [],
   "source": [
    "#with open(\"gdrive/Othercomputers/Mon ordinateur portable/P5_stackoverflow/database_cleaned.pkl\", 'rb') as ifile :\n",
    "with open(\"database_cleaned.pkl\", 'rb') as ifile :\n",
    "    DATA = pickle.load(ifile)\n",
    "with open(\"database_20tags_cleaned.pkl\", 'rb') as ifile :\n",
    "    DATA_20tags = pickle.load(ifile)\n",
    "with open(\"database_50tags_cleaned.pkl\", 'rb') as ifile :\n",
    "    DATA_50tags = pickle.load(ifile)\n",
    "#DATA = pd.read_csv(\"gdrive/Othercomputers/Mon ordinateur portable/P5_stackoverflow/database_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1654863412640,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "b0XQmaU0ml1l",
    "outputId": "44a9b15d-5af9-49fe-8032-ad075128e4de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>Tags_list</th>\n",
       "      <th>Body_words</th>\n",
       "      <th>Title_words</th>\n",
       "      <th>Body_nwords</th>\n",
       "      <th>Body_words_lemmat</th>\n",
       "      <th>Body_nwords_lemmat</th>\n",
       "      <th>Body_words_noSW</th>\n",
       "      <th>Body_words_lemmat_noSW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SQL Server 2008 Full Text Search (FTS) versus ...</td>\n",
       "      <td>&lt;p&gt;I know there have been questions in the pas...</td>\n",
       "      <td>&lt;sql-server&gt;&lt;sql-server-2008&gt;&lt;full-text-search...</td>\n",
       "      <td>499247</td>\n",
       "      <td>40</td>\n",
       "      <td>18582</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>[&lt;sql-server-2008&gt;, &lt;full-text-search&gt;, &lt;lucen...</td>\n",
       "      <td>[i, know, there, have, been, questions, in, th...</td>\n",
       "      <td>[sql, server, 2008, full, text, search, fts, v...</td>\n",
       "      <td>42</td>\n",
       "      <td>[i, know, there, have, be, question, in, the, ...</td>\n",
       "      <td>42</td>\n",
       "      <td>[know, questions, past, sql, versus, lucene, n...</td>\n",
       "      <td>[know, question, past, sql, versus, lucene, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XML Serialization and Inherited Types</td>\n",
       "      <td>&lt;p&gt;Following on from my &lt;a href=\"https://stack...</td>\n",
       "      <td>&lt;c#&gt;&lt;xml&gt;&lt;inheritance&gt;&lt;serialization&gt;&lt;xml-seri...</td>\n",
       "      <td>20084</td>\n",
       "      <td>86</td>\n",
       "      <td>56816</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>[&lt;serialization&gt;, &lt;c#&gt;, &lt;xml&gt;, &lt;inheritance&gt;]</td>\n",
       "      <td>[following, on, from, my, a, href, https, stac...</td>\n",
       "      <td>[xml, serialization, and, inherited, types]</td>\n",
       "      <td>279</td>\n",
       "      <td>[follow, on, from, my, a, href, http, stackove...</td>\n",
       "      <td>279</td>\n",
       "      <td>[following, href, https, stackoverflow, com, q...</td>\n",
       "      <td>[follow, href, http, stackoverflow, com, quest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MyISAM versus InnoDB</td>\n",
       "      <td>&lt;p&gt;I'm working on a projects which involves a ...</td>\n",
       "      <td>&lt;mysql&gt;&lt;database&gt;&lt;performance&gt;&lt;innodb&gt;&lt;myisam&gt;</td>\n",
       "      <td>20148</td>\n",
       "      <td>887</td>\n",
       "      <td>301985</td>\n",
       "      <td>390</td>\n",
       "      <td>25</td>\n",
       "      <td>[&lt;performance&gt;, &lt;database&gt;, &lt;mysql&gt;]</td>\n",
       "      <td>[i, m, working, on, a, projects, which, involv...</td>\n",
       "      <td>[myisam, versus, innodb]</td>\n",
       "      <td>146</td>\n",
       "      <td>[i, m, work, on, a, project, which, involve, a...</td>\n",
       "      <td>146</td>\n",
       "      <td>[working, projects, involves, lot, database, w...</td>\n",
       "      <td>[work, project, involve, lot, database, write,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recommended SQL database design for tags or ta...</td>\n",
       "      <td>&lt;p&gt;I've heard of a few ways to implement taggi...</td>\n",
       "      <td>&lt;sql&gt;&lt;database-design&gt;&lt;tags&gt;&lt;data-modeling&gt;&lt;ta...</td>\n",
       "      <td>20856</td>\n",
       "      <td>325</td>\n",
       "      <td>118552</td>\n",
       "      <td>307</td>\n",
       "      <td>6</td>\n",
       "      <td>[&lt;sql&gt;, &lt;database-design&gt;, &lt;data-modeling&gt;, &lt;t...</td>\n",
       "      <td>[i, ve, heard, of, a, few, ways, to, implement...</td>\n",
       "      <td>[recommended, sql, database, design, for, tags...</td>\n",
       "      <td>82</td>\n",
       "      <td>[i, ve, heard, of, a, few, way, to, implement,...</td>\n",
       "      <td>82</td>\n",
       "      <td>[heard, ways, implement, tagging, using, mappi...</td>\n",
       "      <td>[heard, way, implement, tag, use, mapping, tab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specifying a mySQL ENUM in a Django model</td>\n",
       "      <td>&lt;p&gt;How do I go about specifying and using an E...</td>\n",
       "      <td>&lt;python&gt;&lt;mysql&gt;&lt;django&gt;&lt;django-models&gt;&lt;enums&gt;</td>\n",
       "      <td>21454</td>\n",
       "      <td>99</td>\n",
       "      <td>61572</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>[&lt;django-models&gt;, &lt;python&gt;, &lt;enums&gt;, &lt;django&gt;,...</td>\n",
       "      <td>[how, do, i, go, about, specifying, and, using...</td>\n",
       "      <td>[specifying, a, mysql, enum, in, a, django, mo...</td>\n",
       "      <td>14</td>\n",
       "      <td>[how, do, i, go, about, specify, and, use, an,...</td>\n",
       "      <td>14</td>\n",
       "      <td>[go, specifying, using, enum, django, model]</td>\n",
       "      <td>[go, specify, use, enum, django, model]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  SQL Server 2008 Full Text Search (FTS) versus ...   \n",
       "1              XML Serialization and Inherited Types   \n",
       "2                               MyISAM versus InnoDB   \n",
       "3  Recommended SQL database design for tags or ta...   \n",
       "4          Specifying a mySQL ENUM in a Django model   \n",
       "\n",
       "                                                Body  \\\n",
       "0  <p>I know there have been questions in the pas...   \n",
       "1  <p>Following on from my <a href=\"https://stack...   \n",
       "2  <p>I'm working on a projects which involves a ...   \n",
       "3  <p>I've heard of a few ways to implement taggi...   \n",
       "4  <p>How do I go about specifying and using an E...   \n",
       "\n",
       "                                                Tags      Id  Score  \\\n",
       "0  <sql-server><sql-server-2008><full-text-search...  499247     40   \n",
       "1  <c#><xml><inheritance><serialization><xml-seri...   20084     86   \n",
       "2     <mysql><database><performance><innodb><myisam>   20148    887   \n",
       "3  <sql><database-design><tags><data-modeling><ta...   20856    325   \n",
       "4      <python><mysql><django><django-models><enums>   21454     99   \n",
       "\n",
       "   ViewCount  FavoriteCount  AnswerCount  \\\n",
       "0      18582             26            5   \n",
       "1      56816             42            7   \n",
       "2     301985            390           25   \n",
       "3     118552            307            6   \n",
       "4      61572             21            9   \n",
       "\n",
       "                                           Tags_list  \\\n",
       "0  [<sql-server-2008>, <full-text-search>, <lucen...   \n",
       "1      [<serialization>, <c#>, <xml>, <inheritance>]   \n",
       "2               [<performance>, <database>, <mysql>]   \n",
       "3  [<sql>, <database-design>, <data-modeling>, <t...   \n",
       "4  [<django-models>, <python>, <enums>, <django>,...   \n",
       "\n",
       "                                          Body_words  \\\n",
       "0  [i, know, there, have, been, questions, in, th...   \n",
       "1  [following, on, from, my, a, href, https, stac...   \n",
       "2  [i, m, working, on, a, projects, which, involv...   \n",
       "3  [i, ve, heard, of, a, few, ways, to, implement...   \n",
       "4  [how, do, i, go, about, specifying, and, using...   \n",
       "\n",
       "                                         Title_words  Body_nwords  \\\n",
       "0  [sql, server, 2008, full, text, search, fts, v...           42   \n",
       "1        [xml, serialization, and, inherited, types]          279   \n",
       "2                           [myisam, versus, innodb]          146   \n",
       "3  [recommended, sql, database, design, for, tags...           82   \n",
       "4  [specifying, a, mysql, enum, in, a, django, mo...           14   \n",
       "\n",
       "                                   Body_words_lemmat  Body_nwords_lemmat  \\\n",
       "0  [i, know, there, have, be, question, in, the, ...                  42   \n",
       "1  [follow, on, from, my, a, href, http, stackove...                 279   \n",
       "2  [i, m, work, on, a, project, which, involve, a...                 146   \n",
       "3  [i, ve, heard, of, a, few, way, to, implement,...                  82   \n",
       "4  [how, do, i, go, about, specify, and, use, an,...                  14   \n",
       "\n",
       "                                     Body_words_noSW  \\\n",
       "0  [know, questions, past, sql, versus, lucene, n...   \n",
       "1  [following, href, https, stackoverflow, com, q...   \n",
       "2  [working, projects, involves, lot, database, w...   \n",
       "3  [heard, ways, implement, tagging, using, mappi...   \n",
       "4       [go, specifying, using, enum, django, model]   \n",
       "\n",
       "                              Body_words_lemmat_noSW  \n",
       "0  [know, question, past, sql, versus, lucene, ne...  \n",
       "1  [follow, href, http, stackoverflow, com, quest...  \n",
       "2  [work, project, involve, lot, database, write,...  \n",
       "3  [heard, way, implement, tag, use, mapping, tab...  \n",
       "4            [go, specify, use, enum, django, model]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1654863412953,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "V8sQcXAuoGaB",
    "outputId": "3708de17-49d9-42f9-a29c-13dd352760e4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>Body_nwords</th>\n",
       "      <th>Body_nwords_lemmat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.733800e+04</td>\n",
       "      <td>27338.000000</td>\n",
       "      <td>2.733800e+04</td>\n",
       "      <td>27338.000000</td>\n",
       "      <td>27338.000000</td>\n",
       "      <td>27338.000000</td>\n",
       "      <td>27338.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.652078e+07</td>\n",
       "      <td>113.008523</td>\n",
       "      <td>1.090891e+05</td>\n",
       "      <td>42.724888</td>\n",
       "      <td>7.117236</td>\n",
       "      <td>205.265784</td>\n",
       "      <td>205.265784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.507370e+07</td>\n",
       "      <td>346.850157</td>\n",
       "      <td>2.413333e+05</td>\n",
       "      <td>146.186050</td>\n",
       "      <td>6.735040</td>\n",
       "      <td>230.235349</td>\n",
       "      <td>230.235349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.610000e+02</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.144024e+06</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2.126625e+04</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.180322e+07</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>4.780600e+04</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>139.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.559752e+07</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>1.094415e+05</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.092680e+07</td>\n",
       "      <td>26377.000000</td>\n",
       "      <td>9.893978e+06</td>\n",
       "      <td>11586.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>4192.000000</td>\n",
       "      <td>4192.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id         Score     ViewCount  FavoriteCount   AnswerCount  \\\n",
       "count  2.733800e+04  27338.000000  2.733800e+04   27338.000000  27338.000000   \n",
       "mean   1.652078e+07    113.008523  1.090891e+05      42.724888      7.117236   \n",
       "std    1.507370e+07    346.850157  2.413333e+05     146.186050      6.735040   \n",
       "min    4.000000e+00      6.000000  2.610000e+02      11.000000      1.000000   \n",
       "25%    4.144024e+06     29.000000  2.126625e+04      14.000000      3.000000   \n",
       "50%    1.180322e+07     51.000000  4.780600e+04      19.000000      5.000000   \n",
       "75%    2.559752e+07     98.000000  1.094415e+05      35.000000      9.000000   \n",
       "max    7.092680e+07  26377.000000  9.893978e+06   11586.000000    126.000000   \n",
       "\n",
       "        Body_nwords  Body_nwords_lemmat  \n",
       "count  27338.000000        27338.000000  \n",
       "mean     205.265784          205.265784  \n",
       "std      230.235349          230.235349  \n",
       "min        4.000000            4.000000  \n",
       "25%       79.000000           79.000000  \n",
       "50%      139.000000          139.000000  \n",
       "75%      246.000000          246.000000  \n",
       "max     4192.000000         4192.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRMEqT4Mkb7o"
   },
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8Lesi-mnB1f"
   },
   "source": [
    "### Tokenisation avec lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocIFoQKco114"
   },
   "outputs": [],
   "source": [
    "DATA['Body_sentence_lemmat'] = DATA[\"Body\"].apply(tokenize_lemmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0YVRuAfhGqg"
   },
   "outputs": [],
   "source": [
    "DATA_20tags['Body_sentence_lemmat'] = DATA_20tags[\"Body\"].apply(tokenize_lemmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9hhdQ2NhGqg"
   },
   "outputs": [],
   "source": [
    "DATA_50tags['Body_sentence_lemmat'] = DATA_50tags[\"Body\"].apply(tokenize_lemmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq8SSL0BnJkW"
   },
   "source": [
    "### Tokenisation sans lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxeNWx0jhGqg"
   },
   "outputs": [],
   "source": [
    "DATA_20tags['Body_sentence_nolemmat'] = DATA_20tags[\"Body\"].apply(tokenize_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AMI7MkVhGqg"
   },
   "outputs": [],
   "source": [
    "DATA_50tags['Body_sentence_nolemmat'] = DATA_50tags[\"Body\"].apply(tokenize_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Q51Go1AhGqg"
   },
   "outputs": [],
   "source": [
    "DATA['Body_sentence_nolemmat'] = DATA[\"Body\"].apply(tokenize_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0dPFLLvj1wX"
   },
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBBcKnoPqiym"
   },
   "source": [
    "### CountVectorizer\n",
    "La class countvectorizer consiste à créer une matrice creuse, représentant le nombre d'occurence de chaque mot de vocabulaire dans chaque document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u083MkeemRzj"
   },
   "outputs": [],
   "source": [
    "# Vectorizer \n",
    "cvect = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "cvect_20tags = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "cvect_50tags = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "\n",
    "# fit\n",
    "cv_fit = cvect.fit(DATA[\"Body_sentence_lemmat\"])\n",
    "cv_fit_20tags = cvect.fit(DATA_20tags[\"Body_sentence_lemmat\"])\n",
    "cv_fit_50tags = cvect.fit(DATA_50tags[\"Body_sentence_lemmat\"])\n",
    "\n",
    "# Transformation\n",
    "cv_transform = cvect.transform(DATA[\"Body_sentence_lemmat\"])  \n",
    "cv_transform_20tags = cvect.transform(DATA_20tags[\"Body_sentence_lemmat\"])  \n",
    "cv_transform_50tags = cvect.transform(DATA_50tags[\"Body_sentence_lemmat\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fM0dYEXqlrw"
   },
   "source": [
    "### TF-IDF\n",
    "Est équivalent à un countvectorizer auquel on applique une transformation TF-IDF. \n",
    "La transformation permet d'attribuer un poids différent en fonction de l'importance du mot dans le corpus et de sa fréquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpXGkzvRqnx2"
   },
   "outputs": [],
   "source": [
    "# Vectorizer \n",
    "ctf = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "ctf_20tags = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "ctf_50tags = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "\n",
    "# fit\n",
    "ctf_fit = ctf.fit(DATA[\"Body_sentence_lemmat\"])\n",
    "ctf_fit_20tags = ctf.fit(DATA_20tags[\"Body_sentence_lemmat\"])\n",
    "ctf_fit_50tags = ctf.fit(DATA_50tags[\"Body_sentence_lemmat\"])\n",
    "\n",
    "# Transformation\n",
    "ctf_transform = ctf.transform(DATA[\"Body_sentence_lemmat\"])  \n",
    "ctf_transform_20tags = ctf.transform(DATA_20tags[\"Body_sentence_lemmat\"])  \n",
    "ctf_transform_50tags = ctf.transform(DATA_50tags[\"Body_sentence_lemmat\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iObOpK35rom0"
   },
   "source": [
    "### Sauvegarde des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNgXOKs5v8Im"
   },
   "outputs": [],
   "source": [
    "with open(\"bow_cv.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(cv_transform, ofile)\n",
    "with open(\"bow_tdif.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(ctf_transform, ofile)\n",
    "with open(\"bow_cv_20tags.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(cv_transform_20tags, ofile)\n",
    "with open(\"bow_tdif_20tags.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(ctf_transform_20tags, ofile)\n",
    "with open(\"bow_cv_50tags.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(cv_transform_50tags, ofile)\n",
    "with open(\"bow_tdif_50tags.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(ctf_transform_50tags, ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Jvcr5sGoSDf"
   },
   "source": [
    "## Word2vec\n",
    "Méthode de word embedding développée par Google permettant de créer des features à l'aide d'un réseaux de neuronnes à deux couches.  \n",
    "Il permet de reconstruire le contexte des mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn2Kx-02pUS1"
   },
   "source": [
    "### Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z78Ed3VgobCW"
   },
   "outputs": [],
   "source": [
    "w2v_size=300\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = 24 # adapt to length of sentences\n",
    "sentences = DATA['Body_sentence_lemmat'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]\n",
    "sentences_20tags = DATA_20tags['Body_sentence_lemmat'].to_list()\n",
    "sentences_20tags = [gensim.utils.simple_preprocess(text) for text in sentences_20tags]\n",
    "sentences_50tags = DATA_50tags['Body_sentence_lemmat'].to_list()\n",
    "sentences_50tags = [gensim.utils.simple_preprocess(text) for text in sentences_50tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P7h8AV9vu5e"
   },
   "source": [
    "### modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "executionInfo": {
     "elapsed": 1180900,
     "status": "error",
     "timestamp": 1654853296223,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "i_NRnsIit3HQ",
    "outputId": "f7a572e1-648b-449d-b6f2-7836eee94e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build & train Word2Vec model ...\n",
      "Vocabulary size: 85565\n",
      "Word2Vec trained\n"
     ]
    }
   ],
   "source": [
    "# Création et entraînement du modèle Word2Vec\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
    "                                                vector_size=w2v_size,\n",
    "                                                seed=42,\n",
    "                                                #workers=1)\n",
    "                                                workers=multiprocessing.cpu_count())\n",
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1654853296218,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "7SNzAvGuuhN3",
    "outputId": "b06dde04-f0a8-4e3b-dc99-98425056b688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Tokenizer ...\n",
      "Number of unique words: 85566\n"
     ]
    }
   ],
   "source": [
    "# Préparation des sentences (tokenization)\n",
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') \n",
    "x_sentences_20tags = pad_sequences(tokenizer.texts_to_sequences(sentences_20tags),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') \n",
    "x_sentences_50tags = pad_sequences(tokenizer.texts_to_sequences(sentences_50tags),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') \n",
    "\n",
    "                                                   \n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73H_3yppvqOa"
   },
   "source": [
    "### matrice embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1654853296220,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "4WO1n7C5v-nN",
    "outputId": "aea2c95d-952b-42fa-eab6-a9b969d37acd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Embedding matrix ...\n",
      "Word embedding rate :  1.0\n",
      "Embedding matrix: (85566, 300)\n"
     ]
    }
   ],
   "source": [
    "# Création de la matrice d'embedding\n",
    "\n",
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0\n",
    "    \n",
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]\n",
    "            \n",
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k9XRTFav6PR"
   },
   "source": [
    "### modèle embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1654853296222,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "06Cmw7IKwKcB",
    "outputId": "11e3daf7-c56d-41f8-8775-f0bfb63f35a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 24)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 24, 300)           25669800  \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 300)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,669,800\n",
      "Trainable params: 25,669,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle\n",
    "\n",
    "input=Input(shape=(len(x_sentences),maxlen),dtype='float64')\n",
    "\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')  \n",
    "word_embedding=Embedding(input_dim=vocab_size,\n",
    "                         output_dim=w2v_size,\n",
    "                         weights = [embedding_matrix],\n",
    "                         input_length=maxlen)(word_input)\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)  \n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q03XZpGU42Rn"
   },
   "source": [
    "### création inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwulxLYb46WY",
    "outputId": "baa9d241-070f-4a0b-f8fb-86333bb909c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27338, 300)\n",
      "(21335, 300)\n"
     ]
    }
   ],
   "source": [
    "embeddings_w2v = embed_model.predict(x_sentences)\n",
    "embeddings_w2v_20tags = embed_model.predict(x_sentences_20tags)\n",
    "embeddings_w2v_50tags = embed_model.predict(x_sentences_50tags)\n",
    "print(embeddings_w2v.shape)\n",
    "print(embeddings_w2v_20tags.shape)\n",
    "with open(\"word2vec_features.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(embeddings_w2v, ofile)\n",
    "with open(\"word2vec_20tags_features.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(embeddings_w2v_20tags, ofile)\n",
    "with open(\"word2vec_50tags_features.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(embeddings_w2v_50tags, ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87fwIuHV7t9C"
   },
   "source": [
    "## BERT\n",
    "La méthode Bidirectional Encoder Representations from Transformers est développée par Google.  \n",
    "Le principe consiste à prédire un mot à partir des mots précédents et suivants dans une phrase.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcDa-Fi-7r6V"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TF_KERAS\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1654863796284,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "zMGYNZ-68Lu7",
    "outputId": "7d785f9a-03bf-4166-bb16-1d8a87dda458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "2.8.0\n",
      "Num GPUs Available:  0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6iCued77wly"
   },
   "source": [
    "### fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHGJd9bo8TT7"
   },
   "outputs": [],
   "source": [
    "# Fonction de préparation des sentences\n",
    "def bert_inp_fct(sentences, bert_tokenizer, max_length) :\n",
    "    input_ids=[]\n",
    "    token_type_ids = []\n",
    "    attention_mask=[]\n",
    "    bert_inp_tot = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask = True, \n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "    \n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0], \n",
    "                             bert_inp['token_type_ids'][0], \n",
    "                             bert_inp['attention_mask'][0]))\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "    \n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot\n",
    "    \n",
    "\n",
    "# Fonction de création des features\n",
    "def feature_BERT_fct(model, model_type, sentences, max_length, b_size, mode='HF') :\n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx+batch_size], \n",
    "                                                                      bert_tokenizer, max_length)\n",
    "        \n",
    "        if mode=='HF' :    # Bert HuggingFace\n",
    "            outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if mode=='TFhub' : # Bert Tensorflow Hub\n",
    "            text_preprocessed = {\"input_word_ids\" : input_ids, \n",
    "                                 \"input_mask\" : attention_mask, \n",
    "                                 \"input_type_ids\" : token_type_ids}\n",
    "            outputs = model(text_preprocessed)\n",
    "            last_hidden_states = outputs['sequence_output']\n",
    "             \n",
    "        if step ==0 :\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "            last_hidden_states_tot_0 = last_hidden_states\n",
    "        else :\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot,last_hidden_states))\n",
    "    \n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "    \n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2)\n",
    "     \n",
    "    return features_bert, last_hidden_states_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeAcFVoA8K5Z"
   },
   "source": [
    "### paramètre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zcwd_lds-0kW"
   },
   "outputs": [],
   "source": [
    "#max_length = 64\n",
    "max_length=24\n",
    "batch_size = 5\n",
    "model_type = 'bert-base-uncased'\n",
    "model = TFAutoModel.from_pretrained(model_type)\n",
    "sentences = DATA['Body_sentence_nolemmat'].to_list()\n",
    "sentences_20tags = DATA_20tags['Body_sentence_nolemmat'].to_list()\n",
    "sentences_50tags = DATA_50tags['Body_sentence_nolemmat'].to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b-DYICG_FBb"
   },
   "source": [
    "### creation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUJpWIQVhGql",
    "outputId": "d5d8bdc6-4798-4722-fe7b-8fa0fb01586a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps traitement :  4514.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "model = TFAutoModel.from_pretrained(model_type)\n",
    "features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences, \n",
    "                                                         max_length, batch_size, mode='HF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjqbPeeYhGql",
    "outputId": "bf4618c2-e602-43d2-c5fd-f98c99c38349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27338, 768)\n"
     ]
    }
   ],
   "source": [
    "with open(\"BERT_features.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(features_bert, ofile)\n",
    "print(features_bert.shape)\n",
    "del features_bert, last_hidden_states_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6c28d949515f447ca9f031ccfd678a7c",
      "9059b46700324ce593bb3baa02f54dec",
      "9c17fc39773e457ea9da327dc3364a7f"
     ]
    },
    "id": "KC0BdNyx_Eqe",
    "outputId": "f4e955be-adca-43df-a0d5-37cc887d4d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps traitement :  1469.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "model = TFAutoModel.from_pretrained(model_type)\n",
    "features_bert_20tags, last_hidden_states_tot_20tags = feature_BERT_fct(model, model_type, sentences_20tags, \n",
    "                                                         max_length, batch_size, mode='HF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mk8_0OEohGqm",
    "outputId": "6e61e7ca-7a72-4838-e259-fc605004858e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21335, 768)\n"
     ]
    }
   ],
   "source": [
    "with open(\"BERT_features_20tags.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(features_bert_20tags, ofile)\n",
    "print(features_bert_20tags.shape)\n",
    "del features_bert_20tags, last_hidden_states_tot_20tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSijNLtBhGqm",
    "outputId": "b4fd0ba3-d383-45fe-c42e-941e41794e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps traitement :  5981.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "model = TFAutoModel.from_pretrained(model_type)\n",
    "features_bert_50tags, last_hidden_states_tot_50tags = feature_BERT_fct(model, model_type, sentences_50tags, \n",
    "                                                         max_length, batch_size, mode='HF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I_vv2Usv8Ir",
    "outputId": "1995fd45-72d5-4e23-f35c-7bef9ec1df85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24313, 768)\n"
     ]
    }
   ],
   "source": [
    "with open(\"BERT_features_50tags.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(features_bert_50tags, ofile)\n",
    "print(features_bert_50tags.shape)\n",
    "del features_bert_50tags, last_hidden_states_tot_50tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivFXkL6ehGqm"
   },
   "source": [
    "## BERT HUB tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmJAx6VlvMC5"
   },
   "source": [
    "### Fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1sjPayLhGqm"
   },
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=64):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    \n",
    "    sequence_output = bert_layer({\"input_word_ids\": input_word_ids, \n",
    "                                  \"input_type_ids\":input_type_ids, \n",
    "                                  \"input_mask\" : input_mask})\n",
    "    #print(sequence_output)\n",
    "    clf_output = sequence_output['sequence_output'][:, 0, :]\n",
    "    print(clf_output)\n",
    "    out = Dense(20, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_type_ids, input_mask],\n",
    "                  outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBzk87PB398y"
   },
   "outputs": [],
   "source": [
    "def build_model_notrain(bert_layer, max_len=64):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    \n",
    "    sequence_output = bert_layer({\"input_word_ids\": input_word_ids, \n",
    "                                  \"input_type_ids\":input_type_ids, \n",
    "                                  \"input_mask\" : input_mask})\n",
    "    #print(sequence_output)\n",
    "    clf_output = sequence_output['sequence_output'][:, 0, :]\n",
    "    print(clf_output)\n",
    "    out = Dense(20, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_type_ids, input_mask],\n",
    "                  outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djWFbUrOhGqn"
   },
   "outputs": [],
   "source": [
    "def tokenization(txt) :\n",
    "    tag_tokenizer = nltk.RegexpTokenizer(r'</?(?:b|p)>', gaps=True)\n",
    "    txt_tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    txt = ''.join([i for i in txt if not i.isdigit()])\n",
    "    txt = re.sub(r'_+', ' ', txt)\n",
    "    words = txt_tokenizer.tokenize(' '.join(tag_tokenizer.tokenize(txt.lower())))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKGssQCahGqn"
   },
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x7pdlAMvsRE"
   },
   "source": [
    "### création des inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8dvt613vy8q"
   },
   "outputs": [],
   "source": [
    "train_input = bert_encode(DATA_20tags['Body'], tokenizer, max_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mV89s5xtv1Gv"
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(DATA_20tags['Tags_list'])\n",
    "train_labels = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i0--YLxvPif"
   },
   "source": [
    "### création du modèle avec entrainement uniquement de la couche externe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1655825277332,
     "user": {
      "displayName": "Erwan Chesneau",
      "userId": "06773581090606845368"
     },
     "user_tz": -120
    },
    "id": "u6-2Ls8bhGqq",
    "outputId": "6f543b0a-0ec8-4c11-c54c-a7ea1e5fb30f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-65538e1f3eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
     ]
    }
   ],
   "source": [
    "module_url ='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tV-L0tuhGqq",
    "outputId": "3d780985-836f-4868-e729-fb782aa79486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name='tf.__operators__.getitem_4/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_4'\")\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_mask (InputLayer)        [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_type_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_word_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'sequence_output':  109482241   ['input_mask[0][0]',             \n",
      "                                 (None, 64, 768),                 'input_type_ids[0][0]',         \n",
      "                                 'default': (None,                'input_word_ids[0][0]']         \n",
      "                                768),                                                             \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768),                                                       \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 64, 768),                                                \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768)]}                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['keras_layer_1[0][14]']         \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 20)           15380       ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,497,621\n",
      "Trainable params: 15,380\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_notrain(bert_layer, max_len=64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KBISI8GhGqq",
    "outputId": "dffa8ce3-a73e-49b9-b512-77fca56f20f6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "267/267 [==============================] - 1178s 4s/step - loss: 0.6852 - accuracy: 0.0433 - val_loss: 0.6022 - val_accuracy: 0.0352\n",
      "Epoch 2/5\n",
      "267/267 [==============================] - 1411s 5s/step - loss: 0.5447 - accuracy: 0.0429 - val_loss: 0.4871 - val_accuracy: 0.0363\n",
      "Epoch 3/5\n",
      "267/267 [==============================] - 1587s 6s/step - loss: 0.4510 - accuracy: 0.0431 - val_loss: 0.4106 - val_accuracy: 0.0370\n",
      "Epoch 4/5\n",
      "267/267 [==============================] - 1147s 4s/step - loss: 0.3885 - accuracy: 0.0444 - val_loss: 0.3594 - val_accuracy: 0.0373\n",
      "Epoch 5/5\n",
      "267/267 [==============================] - 1052s 4s/step - loss: 0.3463 - accuracy: 0.0455 - val_loss: 0.3248 - val_accuracy: 0.0368\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4tYBocvhGqq"
   },
   "outputs": [],
   "source": [
    "model.save(\"model_BERT_20tags_len64_notrain.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TdcjgochGqq"
   },
   "outputs": [],
   "source": [
    "cls_layer_model = Model(model.input, outputs=model.get_layer('tf.__operators__.getitem_4').output)\n",
    "BERT_HF_features = cls_layer_model.predict(train_input)\n",
    "with open(\"BERT_HF_features_20tags_notrain.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(BERT_HF_features, ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4nFiQCL1Ilr"
   },
   "source": [
    "### création du modèle totalement sur-entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cYN0m7vhGqn",
    "outputId": "66f65969-fd42-458c-a7c1-c6ed0dbcd529",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erwan\\anaconda3\\lib\\site-packages\\tensorflow_hub\\__init__.py:74: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n",
      "C:\\Users\\erwan\\anaconda3\\lib\\site-packages\\tensorflow_hub\\__init__.py:75: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  distutils.version.LooseVersion(required_tensorflow_version)):\n"
     ]
    }
   ],
   "source": [
    "module_url ='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jn7D_ePbhGqn"
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuHVc5mAhGqo",
    "outputId": "18a37420-1540-4644-faaa-97187e0925ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name='tf.__operators__.getitem_1/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_1'\")\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_mask (InputLayer)        [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_type_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_word_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'sequence_output':  109482241   ['input_mask[0][0]',             \n",
      "                                 (None, 64, 768),                 'input_type_ids[0][0]',         \n",
      "                                 'default': (None,                'input_word_ids[0][0]']         \n",
      "                                768),                                                             \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 64, 768),                                                \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768),                                                 \n",
      "                                 (None, 64, 768)],                                                \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768)}                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['keras_layer[1][14]']           \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 20)           15380       ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,497,621\n",
      "Trainable params: 109,497,620\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erwan\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COSLbuNshGqo",
    "outputId": "e7e10906-b9da-49d4-be41-55d8fdc0fd37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "267/267 [==============================] - 3164s 12s/step - loss: 0.2441 - accuracy: 0.1653 - val_loss: 0.2179 - val_accuracy: 0.2728\n",
      "Epoch 2/5\n",
      "267/267 [==============================] - 3138s 12s/step - loss: 0.2141 - accuracy: 0.2683 - val_loss: 0.2067 - val_accuracy: 0.3103\n",
      "Epoch 3/5\n",
      "267/267 [==============================] - 3421s 13s/step - loss: 0.1991 - accuracy: 0.3246 - val_loss: 0.1906 - val_accuracy: 0.3940\n",
      "Epoch 4/5\n",
      "267/267 [==============================] - 2905s 11s/step - loss: 0.1813 - accuracy: 0.4032 - val_loss: 0.1709 - val_accuracy: 0.4732\n",
      "Epoch 5/5\n",
      "267/267 [==============================] - 2870s 11s/step - loss: 0.1661 - accuracy: 0.4566 - val_loss: 0.1569 - val_accuracy: 0.5062\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_U5MPatdhGqo"
   },
   "outputs": [],
   "source": [
    "model.save(\"model_BERT_20tags_len64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7Ks5GOBhGqo"
   },
   "outputs": [],
   "source": [
    "cls_layer_model = Model(model.input, outputs=model.get_layer('tf.__operators__.getitem_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtstkjachGqo"
   },
   "outputs": [],
   "source": [
    "BERT_HF_features = cls_layer_model.predict(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubJuiqU5hGqo"
   },
   "outputs": [],
   "source": [
    "with open(\"BERT_HF_features_20tags.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(BERT_HF_features, ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJNVeIEiCaU6"
   },
   "source": [
    "## USE\n",
    "Universal Sentence Encoding est une methode d'encodage de phrases universel permettant une classification de phrases ou une recherche de similarité.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJNItfgHCaA4",
    "outputId": "79cc2919-af18-431d-8ff7-29bab71c3bc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erwan\\anaconda3\\lib\\site-packages\\tensorflow_hub\\__init__.py:74: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n",
      "C:\\Users\\erwan\\anaconda3\\lib\\site-packages\\tensorflow_hub\\__init__.py:75: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  distutils.version.LooseVersion(required_tensorflow_version)):\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuNBEgJyH5Uk"
   },
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, b_size) :\n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        feat = embed(sentences[idx:idx+batch_size])\n",
    "\n",
    "        if step ==0 :\n",
    "            features = feat\n",
    "        else :\n",
    "            features = np.concatenate((features,feat))\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsB2MXZRH9RY"
   },
   "outputs": [],
   "source": [
    "sentences = DATA[\"Body_sentence_nolemmat\"].to_list()\n",
    "sentences_20tags = DATA_20tags[\"Body_sentence_nolemmat\"].to_list()\n",
    "sentences_50tags = DATA_50tags[\"Body_sentence_nolemmat\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBSB8AHHIHqX"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "features_USE = feature_USE_fct(sentences, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83qYnnFVhGqz"
   },
   "outputs": [],
   "source": [
    "with open(\"USE_features.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(features_USE, ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5zbOdCrhGqz"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "features_USE_20tags = feature_USE_fct(sentences_20tags, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wLFk16QhGqz"
   },
   "outputs": [],
   "source": [
    "with open(\"USE_features_20tags.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(features_USE_20tags, ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eu0Mud9vhGqz"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "features_USE_50tags = feature_USE_fct(sentences_50tags, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_graNnLv8Ir"
   },
   "outputs": [],
   "source": [
    "with open(\"USE_features_50tags.pkl\", 'wb') as ofile :\n",
    "    pickle.dump(features_USE_50tags, ofile)    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "creation_features.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
